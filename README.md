This project demonstrates building a Retrieval-Augmented Generation (RAG) application. It leverages LangChain for orchestration, ChromaDB for vector storage, and a local GGUF model (TinyLlama) for text generation. The application ingests content from a specified URL, creates embeddings, and then uses a Streamlit interface to allow users to ask questions, with the answers being generated by the RAG pipeline and exposed publicly via Ngrok.
